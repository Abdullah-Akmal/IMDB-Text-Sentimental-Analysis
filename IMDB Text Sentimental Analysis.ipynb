{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab1ae9b",
   "metadata": {},
   "source": [
    "## Kaggle IMDB Dataset of 50k Reviews\n",
    "## Source: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "### Libraries Imported:\n",
    "\n",
    "- **`pandas`**: Used for data loading, manipulation, and analysis, especially in tabular form (DataFrames).\n",
    "\n",
    "- **`re`**: A module for working with regular expressions to clean and transform text (e.g., removing special characters).\n",
    "\n",
    "- **`nltk`**: The Natural Language Toolkit, used for processing human language data. Includes tokenization, stopword removal, and lemmatization.\n",
    "\n",
    "- **`stopwords`** (from `nltk.corpus`): A list of common English words (like \"is\", \"and\", \"the\") that are typically removed to reduce noise in text data.\n",
    "\n",
    "- **`WordNetLemmatizer`** (from `nltk.stem`): Converts words to their base/dictionary form (e.g., \"running\" becomes \"run\"), helping standardize textual input.\n",
    "\n",
    "- **`TfidfVectorizer`** (from `sklearn.feature_extraction.text`): Converts text data into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF), useful for machine learning.\n",
    "\n",
    "- **`train_test_split`** (from `sklearn.model_selection`): Splits the dataset into training and testing sets to evaluate model generalization.\n",
    "\n",
    "- **`LogisticRegression`** (from `sklearn.linear_model`): A popular supervised learning algorithm for binary classification problems like sentiment analysis.\n",
    "\n",
    "- **`MultinomialNB`** (from `sklearn.naive_bayes`): A Naive Bayes algorithm optimized for discrete features, commonly used in text classification.\n",
    "\n",
    "- **`classification_report`, `accuracy_score`, `precision_score`, `recall_score`, `f1_score`** (from `sklearn.metrics`): Evaluation metrics used to assess how well the classification model performs on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c627b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is used for loading, manipulating, and analyzing structured data\n",
    "import pandas as pd\n",
    "\n",
    "# re (Regular Expressions) is used for pattern matching and text cleaning\n",
    "import re\n",
    "\n",
    "# nltk (Natural Language Toolkit) is a powerful Python library for NLP tasks\n",
    "import nltk\n",
    "\n",
    "# Import the list of common English stopwords (e.g., 'the', 'and', 'is') to be removed from text\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# WordNetLemmatizer is used to reduce words to their base/root form (e.g., \"running\" → \"run\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# TfidfVectorizer converts raw text into numerical features based on Term Frequency-Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# train_test_split splits data into training and testing sets for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LogisticRegression is a simple yet powerful linear model for binary classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# MultinomialNB is a Naive Bayes classifier typically used for text classification tasks\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Classification metrics to evaluate model performance using accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a04a9b",
   "metadata": {},
   "source": [
    "## NLTK Resource Downloads\n",
    "\n",
    "The following code downloads essential NLTK resources required for natural language processing tasks:\n",
    "\n",
    "- `stopwords`: A list of common words in various languages that are typically filtered out during text preprocessing.\n",
    "- `wordnet`: A large lexical database of English, used for lemmatization and semantic analysis.\n",
    "- `omw-1.4`: Open Multilingual WordNet, provides translations and links to WordNet in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1081bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the list of stopwords (common words like 'the', 'is', etc. that are usually removed in text processing)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the WordNet lexical database (used for lemmatization, synonym extraction, etc.)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download Open Multilingual Wordnet (helps WordNet support multiple languages)\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296a0d0",
   "metadata": {},
   "source": [
    "## Loading and Previewing the IMDB Dataset\n",
    "\n",
    "We use `pandas` to load the IMDB reviews dataset stored in a CSV file and preview the first few entries:\n",
    "1. **Reading CSV File**: We use `pd.read_csv()` to load the dataset from a specified file path.\n",
    "2. **Viewing Data**: The `.head(10)` function is used to display the first 10 rows of the DataFrame. This helps us get a quick overview of the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb7e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "5  Probably my all-time favorite movie, a story o...  positive\n",
      "6  I sure would like to see a resurrection of a u...  positive\n",
      "7  This show was an amazing, fresh & innovative i...  negative\n",
      "8  Encouraged by the positive comments about this...  negative\n",
      "9  If you like original gut wrenching laughter yo...  positive\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the specified file path into a DataFrame\n",
    "df = pd.read_csv(r\"G:\\Other computers\\My Laptop\\Education and Bootcamp\\Internship\\Developers Hub Internship\\Task 2 Text Sentiment Analysis\\Task_2_IMDB Dataset.csv\")\n",
    "\n",
    "# Display the first five rows of the DataFrame to get a quick overview\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296eda5",
   "metadata": {},
   "source": [
    "## Text Preprocessing Function\n",
    "\n",
    "This function performs several standard preprocessing steps on input text to prepare it for natural language processing tasks such as sentiment analysis or topic modeling.\n",
    "## Preprocessing Steps\n",
    "\n",
    "- **Lowercasing**: Ensures uniformity by converting all characters to lowercase.\n",
    "- **HTML Removal**: Cleans web-based datasets by removing HTML tags.\n",
    "- **Character Filtering**: Removes symbols, punctuation, and any non-alphanumeric characters.\n",
    "- **Tokenization**: Breaks the text into individual words (tokens).\n",
    "- **Stopword Removal**: Filters out common, uninformative words (e.g., \"the\", \"and\", \"is\").\n",
    "- **Lemmatization**: Converts words to their base or dictionary form (e.g., \"running\" → \"run\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74daa8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert all characters to lowercase to ensure uniformity\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags using regex\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # Remove any character that is not a lowercase letter, digit, or whitespace\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Split the text into individual words (tokens)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Load the set of English stopwords (e.g., 'the', 'and', 'is', etc.)\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords from the list of tokens\n",
    "    tokens = [t for t in tokens if t not in stops]\n",
    "    \n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize each token (convert to base form, e.g., 'running' → 'run')\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Join the cleaned tokens back into a single string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4c23c",
   "metadata": {},
   "source": [
    "## Applying Text Preprocessing to the Dataset\n",
    "\n",
    "We apply the `preprocess_text` function to every entry in the `review` column of the DataFrame and store the results in a new column called `clean_review`.\n",
    "### Explanation:\n",
    "- `df['review']`: Accesses the column containing raw review text.\n",
    "- `.apply(preprocess_text)`: Applies the custom preprocessing function to each review.\n",
    "- `df['clean_review']`: Stores the cleaned and normalized version of the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe0b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text preprocessing function to the 'review' column\n",
    "# This creates a new column 'clean_review' with the cleaned text\n",
    "df['clean_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c16a",
   "metadata": {},
   "source": [
    "## Encoding Sentiment Labels\n",
    "\n",
    "We convert the sentiment labels from text to numeric values to make them suitable for machine learning models.\n",
    "### Explanation:\n",
    "- `df['sentiment']`: Accesses the original sentiment column containing `'positive'` or `'negative'` strings.\n",
    "- `.map({'positive': 1, 'negative': 0})`: Maps each sentiment to a corresponding integer (`1` for positive, `0` for negative).\n",
    "- `df['label']`: A new column storing the encoded numeric labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ea8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentiment labels: 'positive' becomes 1, 'negative' becomes 0\n",
    "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927bded",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Testing Sets\n",
    "\n",
    "We use `train_test_split` from `scikit-learn` to divide the dataset into training and testing subsets for model evaluation.\n",
    "### Explanation:\n",
    "- `df['clean_review']`: Input features (preprocessed text).\n",
    "- `df['label']`: Target labels (binary sentiment).\n",
    "- `test_size=0.2`: Reserves 20% of the data for testing.\n",
    "- `random_state=42`: Sets a seed for reproducibility.\n",
    "- `stratify=df['label']`: Ensures class distribution is preserved in both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc42d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_review'],     # Features (preprocessed reviews)\n",
    "    df['label'],            # Target labels (0 or 1)\n",
    "    test_size=0.2,          # 20% of the data will be used for testing\n",
    "    random_state=42,        # Ensures reproducibility of the split\n",
    "    stratify=df['label']    # Keeps the same proportion of classes in both sets\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f45e8",
   "metadata": {},
   "source": [
    "## Feature Engineering with TF-IDF\n",
    "\n",
    "We use `TfidfVectorizer` to convert text data into numerical feature vectors based on Term Frequency–Inverse Document Frequency (TF-IDF), which reflects the importance of words in documents.\n",
    "### Explanation:\n",
    "- `TfidfVectorizer(max_features=10000)`: Limits the vocabulary to the 10,000 most informative words.\n",
    "- `fit_transform(X_train)`: Learns the vocabulary and transforms the training data into a TF-IDF matrix.\n",
    "- `transform(X_test)`: Applies the same vocabulary to transform the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889e0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer with a maximum of 10,000 features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the already-fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e887ed9",
   "metadata": {},
   "source": [
    "## Defining Classification Models\n",
    "\n",
    "We define a dictionary of machine learning models to compare their performance on the text classification task.\n",
    "### Explanation:\n",
    "- `'Logistic Regression'`: Uses `LogisticRegression` with `max_iter=1000` to ensure convergence during training.\n",
    "- `'Multinomial Naive Bayes'`: A probabilistic classifier well-suited for discrete features like word counts or TF-IDF scores.\n",
    "- The models are stored in a dictionary for easy iteration during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a627d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Logistic Regression with increased iterations\n",
    "    'Multinomial Naive Bayes': MultinomialNB()                 # Naive Bayes classifier suitable for text data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e4175",
   "metadata": {},
   "source": [
    "## Model Training, Prediction, and Evaluation\n",
    "\n",
    "We iterate over the defined models, train each one, make predictions, and evaluate their performance using standard classification metrics.\n",
    "### Explanation:\n",
    "- `clf.fit(...)`: Trains the model on TF-IDF features.\n",
    "- `clf.predict(...)`: Predicts labels for the test set.\n",
    "- `accuracy_score`, `precision_score`, `recall_score`, `f1_score`: Measure the quality of predictions.\n",
    "- `classification_report`: Provides detailed performance metrics for each class.\n",
    "- `results[name]`: Stores all metrics in a dictionary for later comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dffc8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "Accuracy : 0.8941\n",
      "Precision: 0.8861\n",
      "Recall   : 0.9044\n",
      "F1-score : 0.8952\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      5000\n",
      "    positive       0.89      0.90      0.90      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "--- Multinomial Naive Bayes ---\n",
      "Accuracy : 0.8584\n",
      "Precision: 0.8568\n",
      "Recall   : 0.8606\n",
      "F1-score : 0.8587\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86      5000\n",
      "    positive       0.86      0.86      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store evaluation results\n",
    "results = {}\n",
    "\n",
    "# Loop through each model in the dictionary\n",
    "for name, clf in models.items():\n",
    "    print(f\"--- {name} ---\")  # Print the model name\n",
    "\n",
    "    # Train the model on the training data\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Predict labels for the test data\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[name] = {\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-score': f1\n",
    "    }\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "    # Print a detailed classification report\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfb2aa",
   "metadata": {},
   "source": [
    "## Summary Comparison of Model Performance\n",
    "\n",
    "This section prints a summary of the evaluation metrics for each model, allowing quick comparison across different classifiers.\n",
    "### Explanation:\n",
    "- `results.items()`: Iterates through each model and its associated performance metrics.\n",
    "- `metrics['Accuracy']`, `['Precision']`, `['Recall']`, `['F1-score']`: Access specific evaluation metrics.\n",
    "- `:.4f`: Formats each metric to 4 decimal places for consistent and readable output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ca74c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "Logistic Regression: Accuracy=0.8941, Precision=0.8861, Recall=0.9044, F1-score=0.8952\n",
      "Multinomial Naive Bayes: Accuracy=0.8584, Precision=0.8568, Recall=0.8606, F1-score=0.8587\n"
     ]
    }
   ],
   "source": [
    "# Print a header for model comparison\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "\n",
    "# Loop through the evaluation results for each model\n",
    "for name, metrics in results.items():\n",
    "    # Print the model's name along with its accuracy, precision, recall, and F1-score\n",
    "    print(f\"{name}: Accuracy={metrics['Accuracy']:.4f}, \"\n",
    "          f\"Precision={metrics['Precision']:.4f}, \"\n",
    "          f\"Recall={metrics['Recall']:.4f}, \"\n",
    "          f\"F1-score={metrics['F1-score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61361544",
   "metadata": {},
   "source": [
    "## Prediction Function (Accepts Both Text and Choice of Model)\n",
    "\n",
    "This function predicts the sentiment of a given text based on the selected machine learning model.\n",
    "### Explanation:\n",
    "- `preprocess_text(text)`: Preprocesses the input text (lowercase, remove stopwords, lemmatization).\n",
    "- `vectorizer.transform([clean])`: Converts the cleaned text into a TF-IDF feature vector.\n",
    "- `model_name not in models`: Checks if the specified model exists in the models dictionary.\n",
    "- `models[model_name].predict(vect)`: Uses the selected model to predict the sentiment based on the transformed vector.\n",
    "- `return 'Positive' if pred == 1 else 'Negative'`: Returns \"Positive\" if the model predicts a 1 (positive sentiment) and \"Negative\" for 0 (negative sentiment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d2de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function that accepts both text and a choice of model\n",
    "def predict_sentiment(text, model_name='Multinomial Naive Bayes'):\n",
    "   \n",
    "    # Preprocess the input text\n",
    "    clean = preprocess_text(text)\n",
    "    \n",
    "    # Transform the cleaned text into a TF-IDF vector\n",
    "    vect = vectorizer.transform([clean])\n",
    "    \n",
    "    # Check if the provided model name exists in the models dictionary\n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Model '{model_name}' not found. Choose from {list(models.keys())}.\")\n",
    "    \n",
    "    # Use the chosen model to predict sentiment and return the result\n",
    "    pred = models[model_name].predict(vect)[0]\n",
    "    \n",
    "    # Return 'Positive' if the prediction is 1, else 'Negative'\n",
    "    return 'Positive' if pred == 1 else 'Negative'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2253a0f",
   "metadata": {},
   "source": [
    "## Building a GUI for Sentiment Analysis\n",
    "\n",
    "This section creates a simple graphical user interface (GUI) using Tkinter, which allows users to input a review, select a model, and analyze sentiment.\n",
    "### Explanation:\n",
    "- **Tkinter Setup**: Initializes the Tkinter GUI framework and creates the main application window (`root`).\n",
    "- **Label for Input**: `tk.Label` is used to create a label (\"Enter Review:\") that explains the purpose of the text box.\n",
    "- **Text Box for Input**: `tk.Text` creates a multi-line text box where users can type the review.\n",
    "- **Model Selection**: A label and `OptionMenu` are used to allow users to select one of the pre-defined models from a dropdown list.\n",
    "- **Grid Layout**: `grid()` positions the widgets (labels, text box, and dropdown) within the window, with padding (`padx`, `pady`) for spacing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6a1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for GUI\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Sentiment Analysis GUI\")  # Set the window title\n",
    "\n",
    "# Input label and text box for entering a review\n",
    "tk.Label(root, text=\"Enter Review:\").grid(row=0, column=0, padx=5, pady=5, sticky=\"w\")  # Label for input\n",
    "input_text = tk.Text(root, height=5, width=60)  # Text box for review input\n",
    "input_text.grid(row=1, column=0, columnspan=2, padx=5, pady=5)  # Place the text box in the window\n",
    "\n",
    "# Model selection label and dropdown menu\n",
    "tk.Label(root, text=\"Select Model:\").grid(row=2, column=0, padx=5, pady=5, sticky=\"w\")  # Label for model selection\n",
    "model_var = tk.StringVar(value=list(models.keys())[0])  # Default model is the first one in the list\n",
    "model_menu = ttk.OptionMenu(root, model_var, list(models.keys())[0], *models.keys())  # Dropdown menu for model selection\n",
    "model_menu.grid(row=2, column=1, padx=5, pady=5, sticky=\"w\")  # Place the dropdown in the window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e07256",
   "metadata": {},
   "source": [
    "## Prediction Result Label and GUI Functionality\n",
    "\n",
    "This section handles the prediction and result display. It creates a button that triggers the sentiment analysis based on the input text.\n",
    "### Explanation:\n",
    "- **`on_predict()`**: This function retrieves the user’s input, calls the sentiment prediction function, and updates the result label with the sentiment (\"Positive\" or \"Negative\").\n",
    "- **`predict_button`**: Creates a button labeled \"Predict Sentiment\" which triggers the `on_predict()` function when clicked.\n",
    "- **`result_label`**: A label that initially displays \"Sentiment: \" and is updated with the prediction after clicking the button.\n",
    "- **`root.mainloop()`**: Starts the Tkinter event loop, making the GUI interactive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b3a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction result label and function to handle prediction\n",
    "def on_predict():\n",
    "    # Get the text entered in the input box (from the first character to the end)\n",
    "    text = input_text.get(\"1.0\", tk.END).strip()\n",
    "    \n",
    "    # If text is entered, predict sentiment using the selected model\n",
    "    if text:\n",
    "        result = predict_sentiment(text, model_var.get())  # Call the prediction function\n",
    "        result_label.config(text=f\"Sentiment: {result}\")  # Update the result label with the prediction\n",
    "    else:\n",
    "        result_label.config(text=\"Please enter text to analyze.\")  # Ask for input if no text is provided\n",
    "\n",
    "# Create the prediction button, bind it to the on_predict function\n",
    "predict_button = tk.Button(root, text=\"Predict Sentiment\", command=on_predict)\n",
    "predict_button.grid(row=3, column=0, columnspan=2, padx=5, pady=5)  # Place the button in the window\n",
    "\n",
    "# Create the result label to display sentiment prediction\n",
    "result_label = tk.Label(root, text=\"Sentiment: \")\n",
    "result_label.grid(row=4, column=0, columnspan=2, padx=5, pady=5)  # Place the label in the window\n",
    "\n",
    "# Start the Tkinter GUI event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
